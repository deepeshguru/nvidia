{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt # must import this although we will not use it explicitly\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('chest_xray/test/NORMAL/')\n",
    "img1 = image.load_img(r'chest_xray/test/NORMAL/' + files[0], target_size=(299, 299))\n",
    "img_array1 = image.img_to_array(img1)\n",
    "img_array_expanded_dims1 = np.expand_dims(img_array1, axis=0)\n",
    "input_img = preprocess_input(img_array_expanded_dims1)\n",
    "\n",
    "for i in files[1:64]:\n",
    "    img2 = image.load_img('chest_xray/test/NORMAL/' + i, target_size=(299, 299))\n",
    "    img_array2 = image.img_to_array(img2)\n",
    "    img_array_expanded_dims2 = np.expand_dims(img_array2, axis=0)\n",
    "    img2 = preprocess_input(img_array_expanded_dims2)\n",
    "\n",
    "    input_img = np.concatenate((input_img, img2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 299, 299, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a \".pb\" model \n",
    "# (can be used to read frozen model or TensorRT model)\n",
    "import time\n",
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 10:30:03.613424 140074735155008 deprecation.py:323] From <ipython-input-4-a0f96aa37027>:5: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.22695040702819824\n",
      "needed time in inference-1:  0.26958417892456055\n",
      "needed time in inference-2:  0.25417065620422363\n",
      "needed time in inference-3:  0.2622084617614746\n",
      "needed time in inference-4:  0.2608671188354492\n",
      "needed time in inference-5:  0.26207971572875977\n",
      "needed time in inference-6:  0.2536125183105469\n",
      "needed time in inference-7:  0.25693774223327637\n",
      "needed time in inference-8:  0.265289306640625\n",
      "needed time in inference-9:  0.25855040550231934\n",
      "needed time in inference-10:  0.2718379497528076\n",
      "needed time in inference-11:  0.26901865005493164\n",
      "needed time in inference-12:  0.27494359016418457\n",
      "needed time in inference-13:  0.2692906856536865\n",
      "needed time in inference-14:  0.267305850982666\n",
      "needed time in inference-15:  0.2747161388397217\n",
      "needed time in inference-16:  0.26824164390563965\n",
      "needed time in inference-17:  0.2651848793029785\n",
      "needed time in inference-18:  0.26915884017944336\n",
      "needed time in inference-19:  0.26670217514038086\n",
      "needed time in inference-20:  0.2688307762145996\n",
      "needed time in inference-21:  0.26754283905029297\n",
      "needed time in inference-22:  0.26248669624328613\n",
      "needed time in inference-23:  0.2693800926208496\n",
      "needed time in inference-24:  0.2631089687347412\n",
      "needed time in inference-25:  0.2597775459289551\n",
      "needed time in inference-26:  0.2564585208892822\n",
      "needed time in inference-27:  0.2780952453613281\n",
      "needed time in inference-28:  0.26547837257385254\n",
      "needed time in inference-29:  0.26227879524230957\n",
      "needed time in inference-30:  0.271101713180542\n",
      "needed time in inference-31:  0.26471972465515137\n",
      "needed time in inference-32:  0.2831878662109375\n",
      "needed time in inference-33:  0.2734968662261963\n",
      "needed time in inference-34:  0.269329309463501\n",
      "needed time in inference-35:  0.270200252532959\n",
      "needed time in inference-36:  0.2694523334503174\n",
      "needed time in inference-37:  0.2659156322479248\n",
      "needed time in inference-38:  0.2722461223602295\n",
      "needed time in inference-39:  0.26421689987182617\n",
      "needed time in inference-40:  0.267484188079834\n",
      "needed time in inference-41:  0.26799654960632324\n",
      "needed time in inference-42:  0.2648797035217285\n",
      "needed time in inference-43:  0.26872968673706055\n",
      "needed time in inference-44:  0.2622523307800293\n",
      "needed time in inference-45:  0.27555274963378906\n",
      "needed time in inference-46:  0.25978970527648926\n",
      "needed time in inference-47:  0.26787567138671875\n",
      "needed time in inference-48:  0.27501797676086426\n",
      "needed time in inference-49:  0.2668302059173584\n",
      "average inference time of fp32:  0.266007285118103\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = 'chest_TensorRT_model_32.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
    "        output = sess.graph.get_tensor_by_name('dense_1/Sigmoid:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred32 = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred_32 = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT_32 = total_time / n_time_inference\n",
    "        print(\"average inference time of fp32: \", avg_time_tensorRT_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.17037320137023926\n",
      "needed time in inference-1:  0.1736588478088379\n",
      "needed time in inference-2:  0.16443347930908203\n",
      "needed time in inference-3:  0.16753911972045898\n",
      "needed time in inference-4:  0.1743161678314209\n",
      "needed time in inference-5:  0.180647611618042\n",
      "needed time in inference-6:  0.16978859901428223\n",
      "needed time in inference-7:  0.16655707359313965\n",
      "needed time in inference-8:  0.16333484649658203\n",
      "needed time in inference-9:  0.16156363487243652\n",
      "needed time in inference-10:  0.163771390914917\n",
      "needed time in inference-11:  0.1631169319152832\n",
      "needed time in inference-12:  0.16362881660461426\n",
      "needed time in inference-13:  0.16409683227539062\n",
      "needed time in inference-14:  0.1652052402496338\n",
      "needed time in inference-15:  0.16817426681518555\n",
      "needed time in inference-16:  0.16233420372009277\n",
      "needed time in inference-17:  0.1789228916168213\n",
      "needed time in inference-18:  0.17987799644470215\n",
      "needed time in inference-19:  0.17692875862121582\n",
      "needed time in inference-20:  0.17852401733398438\n",
      "needed time in inference-21:  0.16909480094909668\n",
      "needed time in inference-22:  0.16197490692138672\n",
      "needed time in inference-23:  0.1626121997833252\n",
      "needed time in inference-24:  0.16288018226623535\n",
      "needed time in inference-25:  0.16827058792114258\n",
      "needed time in inference-26:  0.16515564918518066\n",
      "needed time in inference-27:  0.15967226028442383\n",
      "needed time in inference-28:  0.1558551788330078\n",
      "needed time in inference-29:  0.15673494338989258\n",
      "needed time in inference-30:  0.16417193412780762\n",
      "needed time in inference-31:  0.1585848331451416\n",
      "needed time in inference-32:  0.15699553489685059\n",
      "needed time in inference-33:  0.16366171836853027\n",
      "needed time in inference-34:  0.1658320426940918\n",
      "needed time in inference-35:  0.16259264945983887\n",
      "needed time in inference-36:  0.1607036590576172\n",
      "needed time in inference-37:  0.15897846221923828\n",
      "needed time in inference-38:  0.1589510440826416\n",
      "needed time in inference-39:  0.1649787425994873\n",
      "needed time in inference-40:  0.16069650650024414\n",
      "needed time in inference-41:  0.15587329864501953\n",
      "needed time in inference-42:  0.1605525016784668\n",
      "needed time in inference-43:  0.15778636932373047\n",
      "needed time in inference-44:  0.16408610343933105\n",
      "needed time in inference-45:  0.1725904941558838\n",
      "needed time in inference-46:  0.1649477481842041\n",
      "needed time in inference-47:  0.1650981903076172\n",
      "needed time in inference-48:  0.1615922451019287\n",
      "needed time in inference-49:  0.16516923904418945\n",
      "average inference time of fp16:  0.1652577590942383\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = 'chest_TensorRT_model_16.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
    "        output = sess.graph.get_tensor_by_name('dense_1/Sigmoid:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred16 = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred_16 = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT_16 = total_time / n_time_inference\n",
    "        print(\"average inference time of fp16: \", avg_time_tensorRT_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.1599578857421875\n",
      "needed time in inference-1:  0.16600728034973145\n",
      "needed time in inference-2:  0.1695709228515625\n",
      "needed time in inference-3:  0.17739367485046387\n",
      "needed time in inference-4:  0.17656874656677246\n",
      "needed time in inference-5:  0.1762685775756836\n",
      "needed time in inference-6:  0.17146801948547363\n",
      "needed time in inference-7:  0.16624069213867188\n",
      "needed time in inference-8:  0.16275525093078613\n",
      "needed time in inference-9:  0.16216802597045898\n",
      "needed time in inference-10:  0.1631777286529541\n",
      "needed time in inference-11:  0.16693782806396484\n",
      "needed time in inference-12:  0.16437029838562012\n",
      "needed time in inference-13:  0.16263604164123535\n",
      "needed time in inference-14:  0.16649293899536133\n",
      "needed time in inference-15:  0.169142484664917\n",
      "needed time in inference-16:  0.1664581298828125\n",
      "needed time in inference-17:  0.16732025146484375\n",
      "needed time in inference-18:  0.17411565780639648\n",
      "needed time in inference-19:  0.17079830169677734\n",
      "needed time in inference-20:  0.16874217987060547\n",
      "needed time in inference-21:  0.17104792594909668\n",
      "needed time in inference-22:  0.16950392723083496\n",
      "needed time in inference-23:  0.16688275337219238\n",
      "needed time in inference-24:  0.16324448585510254\n",
      "needed time in inference-25:  0.18151450157165527\n",
      "needed time in inference-26:  0.17823004722595215\n",
      "needed time in inference-27:  0.17653799057006836\n",
      "needed time in inference-28:  0.17492890357971191\n",
      "needed time in inference-29:  0.17236709594726562\n",
      "needed time in inference-30:  0.16835904121398926\n",
      "needed time in inference-31:  0.16647744178771973\n",
      "needed time in inference-32:  0.165787935256958\n",
      "needed time in inference-33:  0.16676926612854004\n",
      "needed time in inference-34:  0.17233991622924805\n",
      "needed time in inference-35:  0.1656632423400879\n",
      "needed time in inference-36:  0.16745376586914062\n",
      "needed time in inference-37:  0.16700387001037598\n",
      "needed time in inference-38:  0.16584038734436035\n",
      "needed time in inference-39:  0.16757655143737793\n",
      "needed time in inference-40:  0.16811251640319824\n",
      "needed time in inference-41:  0.1692495346069336\n",
      "needed time in inference-42:  0.16502976417541504\n",
      "needed time in inference-43:  0.1581099033355713\n",
      "needed time in inference-44:  0.15997910499572754\n",
      "needed time in inference-45:  0.16659951210021973\n",
      "needed time in inference-46:  0.1665194034576416\n",
      "needed time in inference-47:  0.16931867599487305\n",
      "needed time in inference-48:  0.168287992477417\n",
      "needed time in inference-49:  0.16587352752685547\n",
      "average inference time of int8:  0.1682639980316162\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = 'chest_TensorRT_model_88.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
    "        output = sess.graph.get_tensor_by_name('dense_1/Sigmoid:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred8 = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT_8 = total_time / n_time_inference\n",
    "        print(\"average inference time of int8: \", avg_time_tensorRT_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.884479284286499\n",
      "needed time in inference-1:  0.32956910133361816\n",
      "needed time in inference-2:  0.3315303325653076\n",
      "needed time in inference-3:  0.3311593532562256\n",
      "needed time in inference-4:  0.34201574325561523\n",
      "needed time in inference-5:  0.3312687873840332\n",
      "needed time in inference-6:  0.3297877311706543\n",
      "needed time in inference-7:  0.32982468605041504\n",
      "needed time in inference-8:  0.32972049713134766\n",
      "needed time in inference-9:  0.3287022113800049\n",
      "needed time in inference-10:  0.33045101165771484\n",
      "needed time in inference-11:  0.33327817916870117\n",
      "needed time in inference-12:  0.3340446949005127\n",
      "needed time in inference-13:  0.3313601016998291\n",
      "needed time in inference-14:  0.33005762100219727\n",
      "needed time in inference-15:  0.33277034759521484\n",
      "needed time in inference-16:  0.3278679847717285\n",
      "needed time in inference-17:  0.33262109756469727\n",
      "needed time in inference-18:  0.32912135124206543\n",
      "needed time in inference-19:  0.33208584785461426\n",
      "needed time in inference-20:  0.33240365982055664\n",
      "needed time in inference-21:  0.3312203884124756\n",
      "needed time in inference-22:  0.3336033821105957\n",
      "needed time in inference-23:  0.32689571380615234\n",
      "needed time in inference-24:  0.33196330070495605\n",
      "needed time in inference-25:  0.32689571380615234\n",
      "needed time in inference-26:  0.33063530921936035\n",
      "needed time in inference-27:  0.34165143966674805\n",
      "needed time in inference-28:  0.32794761657714844\n",
      "needed time in inference-29:  0.3320126533508301\n",
      "needed time in inference-30:  0.3331491947174072\n",
      "needed time in inference-31:  0.3363323211669922\n",
      "needed time in inference-32:  0.33330249786376953\n",
      "needed time in inference-33:  0.3325183391571045\n",
      "needed time in inference-34:  0.3312807083129883\n",
      "needed time in inference-35:  0.3359382152557373\n",
      "needed time in inference-36:  0.3299851417541504\n",
      "needed time in inference-37:  0.33222055435180664\n",
      "needed time in inference-38:  0.33403658866882324\n",
      "needed time in inference-39:  0.3343980312347412\n",
      "needed time in inference-40:  0.33260297775268555\n",
      "needed time in inference-41:  0.33492469787597656\n",
      "needed time in inference-42:  0.3312065601348877\n",
      "needed time in inference-43:  0.33376502990722656\n",
      "needed time in inference-44:  0.33043980598449707\n",
      "needed time in inference-45:  0.33133459091186523\n",
      "needed time in inference-46:  0.33202171325683594\n",
      "needed time in inference-47:  0.3325057029724121\n",
      "needed time in inference-48:  0.33696579933166504\n",
      "needed time in inference-49:  0.3355443477630615\n",
      "average inference time:  0.3432283592224121\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "FROZEN_MODEL_PATH = 'chest_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
    "        output = sess.graph.get_tensor_by_name('dense_1/Sigmoid:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT improvement compared to the original model: 1.290296839314097\n",
      "TensorRT improvement compared to the original model: 2.076927347336751\n",
      "TensorRT improvement compared to the original model: 2.0398205393759916\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT_32)\n",
    "print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT_16)\n",
    "print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44508702], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(out_pred)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44508702], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(out_pred)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44433767], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(out_pred_16)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.444339], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(out_pred8)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.44508702-0.44433767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.44508702-0.4450868)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput=(1/avg_time_original_model)*64\n",
    "throughput32=(1/avg_time_tensorRT_32)*64\n",
    "throughput16=(1/avg_time_tensorRT_16)*64\n",
    "throughput8=(1/avg_time_tensorRT_8)*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.4647785660624 387.2737979189466 240.5949144271933 380.35468518925023\n"
     ]
    }
   ],
   "source": [
    "print(throughput,throughput16, throughput32, throughput8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_dif32 = sum(out_pred)/64 - sum(out_pred32)/64\n",
    "accu_dif16 = sum(out_pred)/64 - sum(out_pred16)/64\n",
    "accu_dif8 = sum(out_pred)/64 - sum(out_pred8)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0861626e-07] [0.00074935] [0.00074801]\n"
     ]
    }
   ],
   "source": [
    "print(accu_dif32, accu_dif16,accu_dif8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
